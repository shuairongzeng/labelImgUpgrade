#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½è®­ç»ƒè½®æ•°è®¡ç®—å™¨

åŸºäºYOLOè®­ç»ƒæœ€ä½³å®è·µï¼Œæ™ºèƒ½è®¡ç®—æ¨èçš„è®­ç»ƒè½®æ•°
"""

import os
import math
import yaml
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class DatasetInfo:
    """æ•°æ®é›†ä¿¡æ¯"""
    train_images: int
    val_images: int
    total_images: int
    num_classes: int
    dataset_path: str
    config_path: str


@dataclass
class EpochsCalculationResult:
    """è®­ç»ƒè½®æ•°è®¡ç®—ç»“æœ"""
    recommended_epochs: int
    min_epochs: int
    max_epochs: int
    calculation_basis: List[str]
    confidence_level: str  # "é«˜", "ä¸­", "ä½"
    additional_notes: List[str]


class SmartEpochsCalculator:
    """æ™ºèƒ½è®­ç»ƒè½®æ•°è®¡ç®—å™¨"""
    
    def __init__(self):
        # åŸºç¡€å‚æ•°é…ç½®
        self.base_epochs_per_1000_images = 100  # æ¯1000å¼ å›¾ç‰‡çš„åŸºç¡€è½®æ•°
        self.min_epochs = 50   # æœ€å°è®­ç»ƒè½®æ•°
        self.max_epochs = 500  # æœ€å¤§è®­ç»ƒè½®æ•°
        
        # æ¨¡å‹å¤æ‚åº¦ç³»æ•°
        self.model_complexity_factors = {
            'yolov8n': 0.8,   # nanoæ¨¡å‹ï¼Œè®­ç»ƒæ›´å¿«ï¼Œéœ€è¦æ›´å¤šè½®æ•°
            'yolov8s': 1.0,   # smallæ¨¡å‹ï¼ŒåŸºå‡†
            'yolov8m': 1.2,   # mediumæ¨¡å‹ï¼Œæ›´å¤æ‚ï¼Œéœ€è¦æ›´å°‘è½®æ•°
            'yolov8l': 1.4,   # largeæ¨¡å‹ï¼Œæœ€å¤æ‚
            'yolov8x': 1.6,   # extra largeæ¨¡å‹
        }
        
        # æ•°æ®é›†å¤§å°åˆ†ç±»é˜ˆå€¼
        self.dataset_size_thresholds = {
            'very_small': 100,    # æå°æ•°æ®é›†
            'small': 800,         # å°æ•°æ®é›†
            'medium': 3000,       # ä¸­ç­‰æ•°æ®é›†
            'large': 10000,       # å¤§æ•°æ®é›†
        }
    
    def get_dataset_info_from_yaml(self, yaml_path: str) -> Optional[DatasetInfo]:
        """ä»YAMLé…ç½®æ–‡ä»¶è·å–æ•°æ®é›†ä¿¡æ¯"""
        try:
            if not os.path.exists(yaml_path):
                logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")
                return None
            
            with open(yaml_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # è·å–æ•°æ®é›†åŸºç¡€è·¯å¾„
            dataset_base = Path(yaml_path).parent
            
            # è·å–è®­ç»ƒå’ŒéªŒè¯è·¯å¾„
            train_path = dataset_base / config.get('train', 'images/train')
            val_path = dataset_base / config.get('val', 'images/val')
            
            # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
            train_images = self._count_images_in_directory(train_path)
            val_images = self._count_images_in_directory(val_path)
            total_images = train_images + val_images
            
            # è·å–ç±»åˆ«æ•°é‡
            num_classes = config.get('nc', 0)
            
            return DatasetInfo(
                train_images=train_images,
                val_images=val_images,
                total_images=total_images,
                num_classes=num_classes,
                dataset_path=str(dataset_base),
                config_path=yaml_path
            )
            
        except Exception as e:
            logger.error(f"è§£ææ•°æ®é›†é…ç½®å¤±è´¥: {str(e)}")
            return None
    
    def _count_images_in_directory(self, directory: Path) -> int:
        """ç»Ÿè®¡ç›®å½•ä¸­çš„å›¾ç‰‡æ•°é‡"""
        try:
            if not directory.exists():
                return 0
            
            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
            count = 0
            
            for ext in image_extensions:
                count += len(list(directory.glob(f"*{ext}")))
                count += len(list(directory.glob(f"*{ext.upper()}")))
            
            return count
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡å›¾ç‰‡æ•°é‡å¤±è´¥: {str(e)}")
            return 0
    
    def calculate_smart_epochs(self, 
                             dataset_info: DatasetInfo, 
                             model_type: str = 'yolov8n',
                             batch_size: int = 16) -> EpochsCalculationResult:
        """æ™ºèƒ½è®¡ç®—è®­ç»ƒè½®æ•°"""
        try:
            calculation_basis = []
            additional_notes = []
            
            # 1. åŸºäºæ•°æ®é›†å¤§å°çš„åŸºç¡€è®¡ç®—
            base_epochs = self._calculate_base_epochs_by_dataset_size(
                dataset_info.total_images, calculation_basis)
            
            # 2. æ¨¡å‹å¤æ‚åº¦è°ƒæ•´
            model_factor = self.model_complexity_factors.get(model_type.lower(), 1.0)
            adjusted_epochs = int(base_epochs * model_factor)
            calculation_basis.append(f"æ¨¡å‹å¤æ‚åº¦è°ƒæ•´ ({model_type}): {base_epochs} Ã— {model_factor} = {adjusted_epochs}")
            
            # 3. ç±»åˆ«æ•°é‡è°ƒæ•´
            class_adjusted_epochs = self._adjust_for_class_count(
                adjusted_epochs, dataset_info.num_classes, calculation_basis)
            
            # 4. æ•°æ®é›†è´¨é‡è¯„ä¼°è°ƒæ•´
            quality_adjusted_epochs = self._adjust_for_dataset_quality(
                class_adjusted_epochs, dataset_info, calculation_basis)
            
            # 5. æ‰¹æ¬¡å¤§å°è°ƒæ•´
            final_epochs = self._adjust_for_batch_size(
                quality_adjusted_epochs, batch_size, dataset_info.total_images, calculation_basis)
            
            # 6. åº”ç”¨è¾¹ç•Œé™åˆ¶
            final_epochs = max(self.min_epochs, min(self.max_epochs, final_epochs))
            
            # 7. è®¡ç®—æ¨èèŒƒå›´
            min_epochs = max(self.min_epochs, int(final_epochs * 0.7))
            max_epochs = min(self.max_epochs, int(final_epochs * 1.3))
            
            # 8. è¯„ä¼°ç½®ä¿¡åº¦
            confidence_level = self._evaluate_confidence(dataset_info, additional_notes)
            
            # 9. æ·»åŠ é¢å¤–å»ºè®®
            self._add_training_recommendations(dataset_info, model_type, additional_notes)
            
            return EpochsCalculationResult(
                recommended_epochs=final_epochs,
                min_epochs=min_epochs,
                max_epochs=max_epochs,
                calculation_basis=calculation_basis,
                confidence_level=confidence_level,
                additional_notes=additional_notes
            )
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ™ºèƒ½è½®æ•°å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å€¼
            return EpochsCalculationResult(
                recommended_epochs=100,
                min_epochs=50,
                max_epochs=200,
                calculation_basis=[f"è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(e)}"],
                confidence_level="ä½",
                additional_notes=["å»ºè®®æ‰‹åŠ¨è°ƒæ•´å‚æ•°"]
            )
    
    def _calculate_base_epochs_by_dataset_size(self, total_images: int, basis: List[str]) -> int:
        """åŸºäºæ•°æ®é›†å¤§å°è®¡ç®—åŸºç¡€è½®æ•°"""
        if total_images <= self.dataset_size_thresholds['very_small']:
            # æå°æ•°æ®é›†ï¼šéœ€è¦æ›´å¤šè½®æ•°æ¥å……åˆ†å­¦ä¹ 
            base_epochs = 200
            basis.append(f"æå°æ•°æ®é›† ({total_images}å¼ ): åŸºç¡€è½®æ•° {base_epochs}")
        elif total_images <= self.dataset_size_thresholds['small']:
            # å°æ•°æ®é›†ï¼šé€‚ä¸­è½®æ•°
            base_epochs = 150
            basis.append(f"å°æ•°æ®é›† ({total_images}å¼ ): åŸºç¡€è½®æ•° {base_epochs}")
        elif total_images <= self.dataset_size_thresholds['medium']:
            # ä¸­ç­‰æ•°æ®é›†ï¼šæ ‡å‡†è½®æ•°
            base_epochs = 100
            basis.append(f"ä¸­ç­‰æ•°æ®é›† ({total_images}å¼ ): åŸºç¡€è½®æ•° {base_epochs}")
        elif total_images <= self.dataset_size_thresholds['large']:
            # å¤§æ•°æ®é›†ï¼šè¾ƒå°‘è½®æ•°
            base_epochs = 80
            basis.append(f"å¤§æ•°æ®é›† ({total_images}å¼ ): åŸºç¡€è½®æ•° {base_epochs}")
        else:
            # è¶…å¤§æ•°æ®é›†ï¼šæœ€å°‘è½®æ•°
            base_epochs = 60
            basis.append(f"è¶…å¤§æ•°æ®é›† ({total_images}å¼ ): åŸºç¡€è½®æ•° {base_epochs}")
        
        return base_epochs
    
    def _adjust_for_class_count(self, base_epochs: int, num_classes: int, basis: List[str]) -> int:
        """åŸºäºç±»åˆ«æ•°é‡è°ƒæ•´è½®æ•°"""
        if num_classes <= 1:
            # å•ç±»åˆ«æˆ–æœªçŸ¥ç±»åˆ«æ•°
            adjusted = base_epochs
            basis.append(f"ç±»åˆ«æ•°è°ƒæ•´: {num_classes}ç±»ï¼Œæ— è°ƒæ•´")
        elif num_classes <= 5:
            # å°‘ç±»åˆ«ï¼šç¨å¾®å‡å°‘è½®æ•°
            adjusted = int(base_epochs * 0.9)
            basis.append(f"ç±»åˆ«æ•°è°ƒæ•´: {num_classes}ç±»ï¼Œè½®æ•° Ã— 0.9 = {adjusted}")
        elif num_classes <= 20:
            # ä¸­ç­‰ç±»åˆ«æ•°ï¼šæ ‡å‡†
            adjusted = base_epochs
            basis.append(f"ç±»åˆ«æ•°è°ƒæ•´: {num_classes}ç±»ï¼Œæ— è°ƒæ•´")
        else:
            # å¤šç±»åˆ«ï¼šå¢åŠ è½®æ•°
            adjusted = int(base_epochs * 1.2)
            basis.append(f"ç±»åˆ«æ•°è°ƒæ•´: {num_classes}ç±»ï¼Œè½®æ•° Ã— 1.2 = {adjusted}")
        
        return adjusted
    
    def _adjust_for_dataset_quality(self, base_epochs: int, dataset_info: DatasetInfo, basis: List[str]) -> int:
        """åŸºäºæ•°æ®é›†è´¨é‡è°ƒæ•´è½®æ•°"""
        # è®¡ç®—è®­ç»ƒ/éªŒè¯æ¯”ä¾‹
        if dataset_info.total_images == 0:
            return base_epochs
        
        train_ratio = dataset_info.train_images / dataset_info.total_images
        
        if train_ratio < 0.6:
            # è®­ç»ƒæ•°æ®å¤ªå°‘ï¼Œéœ€è¦æ›´å¤šè½®æ•°
            adjusted = int(base_epochs * 1.3)
            basis.append(f"æ•°æ®è´¨é‡è°ƒæ•´: è®­ç»ƒæ¯”ä¾‹è¿‡ä½({train_ratio:.1%})ï¼Œè½®æ•° Ã— 1.3 = {adjusted}")
        elif train_ratio > 0.9:
            # éªŒè¯æ•°æ®å¤ªå°‘ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼Œå‡å°‘è½®æ•°
            adjusted = int(base_epochs * 0.8)
            basis.append(f"æ•°æ®è´¨é‡è°ƒæ•´: éªŒè¯æ¯”ä¾‹è¿‡ä½({1-train_ratio:.1%})ï¼Œè½®æ•° Ã— 0.8 = {adjusted}")
        else:
            # æ¯”ä¾‹åˆç†
            adjusted = base_epochs
            basis.append(f"æ•°æ®è´¨é‡è°ƒæ•´: è®­ç»ƒ/éªŒè¯æ¯”ä¾‹åˆç†({train_ratio:.1%})ï¼Œæ— è°ƒæ•´")
        
        return adjusted
    
    def _adjust_for_batch_size(self, base_epochs: int, batch_size: int, total_images: int, basis: List[str]) -> int:
        """åŸºäºæ‰¹æ¬¡å¤§å°è°ƒæ•´è½®æ•°"""
        if total_images == 0:
            return base_epochs
        
        # è®¡ç®—æ¯ä¸ªepochçš„è¿­ä»£æ¬¡æ•°
        iterations_per_epoch = max(1, total_images // batch_size)
        
        if iterations_per_epoch < 10:
            # æ¯ä¸ªepochè¿­ä»£æ¬¡æ•°å¤ªå°‘ï¼Œéœ€è¦æ›´å¤šè½®æ•°
            adjusted = int(base_epochs * 1.5)
            basis.append(f"æ‰¹æ¬¡å¤§å°è°ƒæ•´: æ¯è½®è¿­ä»£è¿‡å°‘({iterations_per_epoch}æ¬¡)ï¼Œè½®æ•° Ã— 1.5 = {adjusted}")
        elif iterations_per_epoch > 100:
            # æ¯ä¸ªepochè¿­ä»£æ¬¡æ•°å¾ˆå¤šï¼Œå¯ä»¥å‡å°‘è½®æ•°
            adjusted = int(base_epochs * 0.8)
            basis.append(f"æ‰¹æ¬¡å¤§å°è°ƒæ•´: æ¯è½®è¿­ä»£è¾ƒå¤š({iterations_per_epoch}æ¬¡)ï¼Œè½®æ•° Ã— 0.8 = {adjusted}")
        else:
            # è¿­ä»£æ¬¡æ•°åˆç†
            adjusted = base_epochs
            basis.append(f"æ‰¹æ¬¡å¤§å°è°ƒæ•´: æ¯è½®è¿­ä»£åˆç†({iterations_per_epoch}æ¬¡)ï¼Œæ— è°ƒæ•´")
        
        return adjusted
    
    def _evaluate_confidence(self, dataset_info: DatasetInfo, notes: List[str]) -> str:
        """è¯„ä¼°è®¡ç®—ç»“æœçš„ç½®ä¿¡åº¦"""
        confidence_score = 0
        
        # æ•°æ®é›†å¤§å°è¯„åˆ†
        if dataset_info.total_images >= 1000:
            confidence_score += 3
        elif dataset_info.total_images >= 200:
            confidence_score += 2
        else:
            confidence_score += 1
            notes.append("æ•°æ®é›†è¾ƒå°ï¼Œå»ºè®®å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
        
        # ç±»åˆ«æ•°é‡è¯„åˆ†
        if 2 <= dataset_info.num_classes <= 50:
            confidence_score += 2
        else:
            confidence_score += 1
            if dataset_info.num_classes > 50:
                notes.append("ç±»åˆ«æ•°é‡è¾ƒå¤šï¼Œå¯èƒ½éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´")
        
        # æ•°æ®åˆ†å¸ƒè¯„åˆ†
        if dataset_info.train_images > 0 and dataset_info.val_images > 0:
            train_ratio = dataset_info.train_images / dataset_info.total_images
            if 0.7 <= train_ratio <= 0.8:
                confidence_score += 2
            else:
                confidence_score += 1
                notes.append("å»ºè®®è°ƒæ•´è®­ç»ƒ/éªŒè¯æ•°æ®æ¯”ä¾‹è‡³7:3æˆ–8:2")
        
        # è¿”å›ç½®ä¿¡åº¦ç­‰çº§
        if confidence_score >= 6:
            return "é«˜"
        elif confidence_score >= 4:
            return "ä¸­"
        else:
            return "ä½"
    
    def _add_training_recommendations(self, dataset_info: DatasetInfo, model_type: str, notes: List[str]):
        """æ·»åŠ è®­ç»ƒå»ºè®®"""
        # æ•°æ®é›†å¤§å°ç›¸å…³å»ºè®®
        if dataset_info.total_images < 100:
            notes.append("âš ï¸ æ•°æ®é›†è¿‡å°ï¼Œå»ºè®®ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯")
            notes.append("ğŸ’¡ è€ƒè™‘ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ")
        
        # æ¨¡å‹é€‰æ‹©å»ºè®®
        if dataset_info.total_images < 500 and model_type in ['yolov8l', 'yolov8x']:
            notes.append("âš ï¸ æ•°æ®é›†è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹(yolov8n/s)é¿å…è¿‡æ‹Ÿåˆ")
        
        # éªŒè¯æ•°æ®å»ºè®®
        if dataset_info.val_images < 50:
            notes.append("âš ï¸ éªŒè¯æ•°æ®è¿‡å°‘ï¼Œå¯èƒ½æ— æ³•å‡†ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½")
        
        # ç±»åˆ«å¹³è¡¡å»ºè®®
        if dataset_info.num_classes > 20:
            notes.append("ğŸ’¡ å¤šç±»åˆ«æ£€æµ‹ï¼Œå»ºè®®ç›‘æ§å„ç±»åˆ«çš„è®­ç»ƒæ•ˆæœ")
